{"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":2916.765375,"end_time":"2024-06-01T20:01:00.218279","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-06-01T19:12:23.452904","version":"2.5.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport math\nimport numpy as np\n\nclass ChebyshevKANLayer(nn.Module):\n    def __init__(self, input_dim, output_dim, degree):\n        super(ChebyshevKANLayer, self).__init__()\n        self.inputdim = input_dim\n        self.outdim = output_dim\n        self.degree = degree\n\n        self.cheby_coeffs = nn.Parameter(torch.empty(input_dim, output_dim, degree + 1))\n        nn.init.xavier_normal_(self.cheby_coeffs)\n        self.register_buffer(\"arange\", torch.arange(0, degree + 1, 1))\n\n    def chebyshev_polynomials(self, x):\n        T = [torch.ones_like(x), x]\n        for n in range(2, self.degree + 1):\n            T.append(2 * x * T[n - 1] - T[n - 2])\n        return torch.stack(T, dim=-1)\n\n    def forward(self, x):\n        x = x.view(-1, self.inputdim)\n        x = 2 * (x - x.min(dim=1, keepdim=True)[0]) / (x.max(dim=1, keepdim=True)[0] - x.min(dim=1, keepdim=True)[0]) - 1\n        T = self.chebyshev_polynomials(x)\n        y = torch.einsum(\"bij,ioj->bo\", T, self.cheby_coeffs)\n        y = y.view(-1, self.outdim)\n        return y\n\nclass KAN(nn.Module):\n    def __init__(self, layers_hidden, degree=3):\n        super(KAN, self).__init__()\n        self.layers = nn.ModuleList()\n        for in_features, out_features in zip(layers_hidden, layers_hidden[1:]):\n            self.layers.append(ChebyshevKANLayer(in_features, out_features, degree))\n\n    def forward(self, x: torch.Tensor):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\nclass MultiheadKANAttention(nn.Module):\n    def __init__(self, hidden_size, num_heads, rotation_matrix, degree=3):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.head_dim = hidden_size // num_heads\n        self.position_emb = rotation_matrix\n\n        self.qkv_linear = ChebyshevKANLayer(hidden_size, hidden_size * 3, degree)\n        self.out = nn.Linear(hidden_size, hidden_size)\n\n    def forward(self, x):\n        batch_size, seq_length, hidden_size = x.size()\n        qkv = self.qkv_linear(x)\n        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n        qkv = qkv.transpose(1, 2)\n        queries, keys, values = qkv.chunk(3, dim=-1)\n        queries = apply_rotary_pos_emb(queries, self.position_emb)\n        keys = apply_rotary_pos_emb(keys, self.position_emb)\n        scores = torch.matmul(queries, keys.transpose(2, 3))\n        scores = scores / (self.head_dim ** 0.5)\n        attention = F.softmax(scores, dim=-1)\n        context = torch.matmul(attention, values)\n        context = context.transpose(1, 2)\n        context = context.reshape(batch_size, seq_length, hidden_size)\n        output = self.out(context)\n        return output\n\nclass KANFormer(nn.Module):\n    def __init__(self, num_features, hidden_size, num_heads, n_blocks, ff_dims, max_seq_len, device, degree=3):\n        super().__init__()\n        self.embedding = nn.Linear(num_features, hidden_size)\n        head_dim = hidden_size // num_heads\n        rope = RotaryPositionalEmbedding(head_dim, max_seq_len)\n        rotation_matrix = rope(max_seq_len).to(device)\n        self.blocks = nn.ModuleList([KANBlock(hidden_size, num_heads, rotation_matrix, degree) for _ in range(n_blocks)])\n        self.ff = nn.ModuleList()\n        in_size = max_seq_len * hidden_size\n        for f in ff_dims:\n            self.ff.append(ChebyshevKANLayer(in_size, f, degree))\n            in_size = f\n\n    def forward(self, x):\n        x = self.embedding(x)\n        for block in self.blocks:\n            x = block(x)\n        x = x.flatten(start_dim=1)\n        for f in self.ff:\n            x = f(x)\n        return x\n\nclass KANBlock(nn.Module):\n    def __init__(self, hidden_size, num_heads, rotation_matrix, degree=3):\n        super().__init__()\n        self.norm1 = RMSNorm(hidden_size)\n        self.attention = MultiheadKANAttention(hidden_size, num_heads, rotation_matrix, degree)\n\n    def forward(self, x):\n        x1 = self.attention(self.norm1(x))\n        out = x + x1\n        return out\n\nclass RMSNorm(nn.Module):\n    def __init__(self, hidden_size: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight\n\nclass RotaryPositionalEmbedding(nn.Module):\n    def __init__(self, dim, max_seq_len):\n        super(RotaryPositionalEmbedding, self).__init__()\n        self.dim = dim\n        self.max_seq_len = max_seq_len\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n        self.register_buffer('pos_enc', self._generate_positional_encoding(max_seq_len))\n\n    def _generate_positional_encoding(self, seq_len):\n        t = torch.arange(seq_len, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n        pos_enc = torch.cat((freqs.sin(), freqs.cos()), dim=-1)\n        return pos_enc\n\n    def forward(self, seq_len):\n        return self.pos_enc[:seq_len, :]\n\ndef apply_rotary_pos_emb(x, pos_emb):\n    x_cos, x_sin = torch.split(pos_emb, x.shape[-1] // 2, dim=-1)\n    x1_rot = (x[..., ::2] * x_cos) + (rotate_half(x[..., 1::2]) * x_sin)\n    x2_rot = (x[..., 1::2] * x_cos) + (rotate_half(x[..., ::2]) * x_sin)\n    x_rot = torch.cat([x1_rot, x2_rot], dim=-1)\n    return x_rot\n\ndef rotate_half(x):\n    x1, x2 = torch.chunk(x, 2, dim=-1)\n    return torch.cat((-x2, x1), dim=-1)\n","metadata":{"execution":{"iopub.execute_input":"2024-06-01T19:42:37.749929Z","iopub.status.busy":"2024-06-01T19:42:37.749097Z","iopub.status.idle":"2024-06-01T19:42:37.789352Z","shell.execute_reply":"2024-06-01T19:42:37.788225Z"},"papermill":{"duration":0.060849,"end_time":"2024-06-01T19:42:37.791954","exception":false,"start_time":"2024-06-01T19:42:37.731105","status":"completed"},"tags":[]},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"start=time.time()\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport torch.optim as optim\n\n# Define the transformations for the MNIST dataset\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# Load the MNIST dataset\ntrain_dataset = datasets.MNIST('.', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST('.', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n\n# Adjust the model to handle the MNIST input size\nclass KANFormerMNIST(KANFormer):\n    def __init__(self, hidden_size, num_heads, n_blocks, ff_dims, max_seq_len, device, degree=3):\n        super().__init__(28, hidden_size, num_heads, n_blocks, ff_dims, max_seq_len, device, degree)\n\n    def forward(self, x):\n        # Flatten the input images and treat each row as a sequence\n        x = x.view(x.size(0), 28, 28)\n        return super().forward(x)\n\n# Initialize the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = KANFormerMNIST(64, 8, 4, [128, 64], 28, device, 3).to(device)\n\n# Define the optimizer and loss function\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Train the model\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 100 == 0:\n            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n\n# Evaluate the model\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += criterion(output, target).item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n          f'({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n\n# Run the training and testing loop\nfor epoch in range(1, 11):\n    train(model, device, train_loader, optimizer, epoch)\n    test(model, device, test_loader)\nstop=time.time()\nprint(stop-start)","metadata":{"execution":{"iopub.execute_input":"2024-06-01T19:42:37.825805Z","iopub.status.busy":"2024-06-01T19:42:37.825360Z","iopub.status.idle":"2024-06-01T20:00:59.008075Z","shell.execute_reply":"2024-06-01T20:00:59.006686Z"},"papermill":{"duration":1101.202356,"end_time":"2024-06-01T20:00:59.010695","exception":false,"start_time":"2024-06-01T19:42:37.808339","status":"completed"},"tags":[]},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"Train Epoch: 1 [0/60000 (0%)]\tLoss: 4.687955\n\nTrain Epoch: 1 [6400/60000 (11%)]\tLoss: 0.909678\n\nTrain Epoch: 1 [12800/60000 (21%)]\tLoss: 0.352214\n\nTrain Epoch: 1 [19200/60000 (32%)]\tLoss: 0.335274\n\nTrain Epoch: 1 [25600/60000 (43%)]\tLoss: 0.170670\n\nTrain Epoch: 1 [32000/60000 (53%)]\tLoss: 0.228204\n\nTrain Epoch: 1 [38400/60000 (64%)]\tLoss: 0.299036\n\nTrain Epoch: 1 [44800/60000 (75%)]\tLoss: 0.136015\n\nTrain Epoch: 1 [51200/60000 (85%)]\tLoss: 0.182974\n\nTrain Epoch: 1 [57600/60000 (96%)]\tLoss: 0.153536\n\n\n\nTest set: Average loss: 0.0002, Accuracy: 9564/10000 (96%)\n\n\n\nTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.123692\n\nTrain Epoch: 2 [6400/60000 (11%)]\tLoss: 0.191082\n\nTrain Epoch: 2 [12800/60000 (21%)]\tLoss: 0.107106\n\nTrain Epoch: 2 [19200/60000 (32%)]\tLoss: 0.185987\n\nTrain Epoch: 2 [25600/60000 (43%)]\tLoss: 0.307873\n\nTrain Epoch: 2 [32000/60000 (53%)]\tLoss: 0.182048\n\nTrain Epoch: 2 [38400/60000 (64%)]\tLoss: 0.125639\n\nTrain Epoch: 2 [44800/60000 (75%)]\tLoss: 0.141397\n\nTrain Epoch: 2 [51200/60000 (85%)]\tLoss: 0.113503\n\nTrain Epoch: 2 [57600/60000 (96%)]\tLoss: 0.145896\n\n\n\nTest set: Average loss: 0.0001, Accuracy: 9636/10000 (96%)\n\n\n\nTrain Epoch: 3 [0/60000 (0%)]\tLoss: 0.115301\n\nTrain Epoch: 3 [6400/60000 (11%)]\tLoss: 0.141522\n\nTrain Epoch: 3 [12800/60000 (21%)]\tLoss: 0.148478\n\nTrain Epoch: 3 [19200/60000 (32%)]\tLoss: 0.050796\n\nTrain Epoch: 3 [25600/60000 (43%)]\tLoss: 0.052406\n\nTrain Epoch: 3 [32000/60000 (53%)]\tLoss: 0.104966\n\nTrain Epoch: 3 [38400/60000 (64%)]\tLoss: 0.059898\n\nTrain Epoch: 3 [44800/60000 (75%)]\tLoss: 0.027964\n\nTrain Epoch: 3 [51200/60000 (85%)]\tLoss: 0.137636\n\nTrain Epoch: 3 [57600/60000 (96%)]\tLoss: 0.045435\n\n\n\nTest set: Average loss: 0.0001, Accuracy: 9707/10000 (97%)\n\n\n\nTrain Epoch: 4 [0/60000 (0%)]\tLoss: 0.017041\n\nTrain Epoch: 4 [6400/60000 (11%)]\tLoss: 0.025651\n\nTrain Epoch: 4 [12800/60000 (21%)]\tLoss: 0.028417\n\nTrain Epoch: 4 [19200/60000 (32%)]\tLoss: 0.022161\n\nTrain Epoch: 4 [25600/60000 (43%)]\tLoss: 0.041872\n\nTrain Epoch: 4 [32000/60000 (53%)]\tLoss: 0.013073\n\nTrain Epoch: 4 [38400/60000 (64%)]\tLoss: 0.105639\n\nTrain Epoch: 4 [44800/60000 (75%)]\tLoss: 0.148230\n\nTrain Epoch: 4 [51200/60000 (85%)]\tLoss: 0.095725\n\nTrain Epoch: 4 [57600/60000 (96%)]\tLoss: 0.116106\n\n\n\nTest set: Average loss: 0.0001, Accuracy: 9750/10000 (98%)\n\n\n\nTrain Epoch: 5 [0/60000 (0%)]\tLoss: 0.020161\n\nTrain Epoch: 5 [6400/60000 (11%)]\tLoss: 0.116391\n\nTrain Epoch: 5 [12800/60000 (21%)]\tLoss: 0.025164\n\nTrain Epoch: 5 [19200/60000 (32%)]\tLoss: 0.025127\n\nTrain Epoch: 5 [25600/60000 (43%)]\tLoss: 0.067285\n\nTrain Epoch: 5 [32000/60000 (53%)]\tLoss: 0.112605\n\nTrain Epoch: 5 [38400/60000 (64%)]\tLoss: 0.042717\n\nTrain Epoch: 5 [44800/60000 (75%)]\tLoss: 0.199054\n\nTrain Epoch: 5 [51200/60000 (85%)]\tLoss: 0.010676\n\nTrain Epoch: 5 [57600/60000 (96%)]\tLoss: 0.044745\n\n\n\nTest set: Average loss: 0.0001, Accuracy: 9783/10000 (98%)\n\n\n\nTrain Epoch: 6 [0/60000 (0%)]\tLoss: 0.020820\n\nTrain Epoch: 6 [6400/60000 (11%)]\tLoss: 0.021499\n\nTrain Epoch: 6 [12800/60000 (21%)]\tLoss: 0.036462\n\nTrain Epoch: 6 [19200/60000 (32%)]\tLoss: 0.020050\n\nTrain Epoch: 6 [25600/60000 (43%)]\tLoss: 0.031200\n\nTrain Epoch: 6 [32000/60000 (53%)]\tLoss: 0.017926\n\nTrain Epoch: 6 [38400/60000 (64%)]\tLoss: 0.069994\n\nTrain Epoch: 6 [44800/60000 (75%)]\tLoss: 0.034714\n\nTrain Epoch: 6 [51200/60000 (85%)]\tLoss: 0.057719\n\nTrain Epoch: 6 [57600/60000 (96%)]\tLoss: 0.034385\n\n\n\nTest set: Average loss: 0.0001, Accuracy: 9804/10000 (98%)\n\n\n\nTrain Epoch: 7 [0/60000 (0%)]\tLoss: 0.034417\n\nTrain Epoch: 7 [6400/60000 (11%)]\tLoss: 0.015683\n\nTrain Epoch: 7 [12800/60000 (21%)]\tLoss: 0.020865\n\nTrain Epoch: 7 [19200/60000 (32%)]\tLoss: 0.086307\n\nTrain Epoch: 7 [25600/60000 (43%)]\tLoss: 0.036210\n\nTrain Epoch: 7 [32000/60000 (53%)]\tLoss: 0.036596\n\nTrain Epoch: 7 [38400/60000 (64%)]\tLoss: 0.051587\n\nTrain Epoch: 7 [44800/60000 (75%)]\tLoss: 0.078354\n\nTrain Epoch: 7 [51200/60000 (85%)]\tLoss: 0.066783\n\nTrain Epoch: 7 [57600/60000 (96%)]\tLoss: 0.006044\n\n\n\nTest set: Average loss: 0.0001, Accuracy: 9778/10000 (98%)\n\n\n\nTrain Epoch: 8 [0/60000 (0%)]\tLoss: 0.103466\n\nTrain Epoch: 8 [6400/60000 (11%)]\tLoss: 0.020827\n\nTrain Epoch: 8 [12800/60000 (21%)]\tLoss: 0.011651\n\nTrain Epoch: 8 [19200/60000 (32%)]\tLoss: 0.087794\n\nTrain Epoch: 8 [25600/60000 (43%)]\tLoss: 0.064852\n\nTrain Epoch: 8 [32000/60000 (53%)]\tLoss: 0.074789\n\nTrain Epoch: 8 [38400/60000 (64%)]\tLoss: 0.031629\n\nTrain Epoch: 8 [44800/60000 (75%)]\tLoss: 0.042493\n\nTrain Epoch: 8 [51200/60000 (85%)]\tLoss: 0.108663\n\nTrain Epoch: 8 [57600/60000 (96%)]\tLoss: 0.100358\n\n\n\nTest set: Average loss: 0.0001, Accuracy: 9805/10000 (98%)\n\n\n\nTrain Epoch: 9 [0/60000 (0%)]\tLoss: 0.022560\n\nTrain Epoch: 9 [6400/60000 (11%)]\tLoss: 0.010220\n\nTrain Epoch: 9 [12800/60000 (21%)]\tLoss: 0.008638\n\nTrain Epoch: 9 [19200/60000 (32%)]\tLoss: 0.056233\n\nTrain Epoch: 9 [25600/60000 (43%)]\tLoss: 0.009548\n\nTrain Epoch: 9 [32000/60000 (53%)]\tLoss: 0.020004\n\nTrain Epoch: 9 [38400/60000 (64%)]\tLoss: 0.018289\n\nTrain Epoch: 9 [44800/60000 (75%)]\tLoss: 0.008105\n\nTrain Epoch: 9 [51200/60000 (85%)]\tLoss: 0.028400\n\nTrain Epoch: 9 [57600/60000 (96%)]\tLoss: 0.019197\n\n\n\nTest set: Average loss: 0.0001, Accuracy: 9802/10000 (98%)\n\n\n\nTrain Epoch: 10 [0/60000 (0%)]\tLoss: 0.072055\n\nTrain Epoch: 10 [6400/60000 (11%)]\tLoss: 0.026544\n\nTrain Epoch: 10 [12800/60000 (21%)]\tLoss: 0.043525\n\nTrain Epoch: 10 [19200/60000 (32%)]\tLoss: 0.013633\n\nTrain Epoch: 10 [25600/60000 (43%)]\tLoss: 0.011863\n\nTrain Epoch: 10 [32000/60000 (53%)]\tLoss: 0.028358\n\nTrain Epoch: 10 [38400/60000 (64%)]\tLoss: 0.011394\n\nTrain Epoch: 10 [44800/60000 (75%)]\tLoss: 0.016417\n\nTrain Epoch: 10 [51200/60000 (85%)]\tLoss: 0.064851\n\nTrain Epoch: 10 [57600/60000 (96%)]\tLoss: 0.095931\n\n\n\nTest set: Average loss: 0.0001, Accuracy: 9823/10000 (98%)\n\n\n\n1101.1623513698578\n"}]},{"cell_type":"markdown","source":"#### The model is not optimized","metadata":{}}]}